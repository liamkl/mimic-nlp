{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BST 261: Final Project\n",
    "\n",
    "## Subtitle\n",
    "\n",
    "\n",
    "### 1. Overview \n",
    "\n",
    "\n",
    "#### 1.1 Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medical coding is a multibillion dollar industry which is highly labor intensive and prone to error. This presents an opportunity for Natural Language Processing (NLP).\n",
    "\n",
    "ICD-9 codes were created by CMS (Centers for Medicare and Medicaid Services) to standardize the way in which patients health outcomes were categorized and tracked over time. They can entered into a patient's electronic health record and can be used for diagnostic, billing and reporting purposes. Currently identifying an ICD-9 code is an manual process, which is slow, expensive, and error prone. Creating a model that could automate the prediction of ICD-9 codes off of doctor's notes would be very beneficial.\n",
    "\n",
    "The specific scope of our project is to predict the 5 most common ICD-9 codes from doctor discharge notes collected from ICU stays between 2001 and 2011 dates at the Beth Israel Deaconess Medical Center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated ICD-9 codes assignment has been studied since 1990. Previous work has been focussed on pattern matching, rule base systems, or supervised classification methods such as Logistic Regression, k-NN and Support Vector Machines.\n",
    "\n",
    "Manual methods and supervised have shown good performance for specific sets of codes and data sets, however these do not generalize well.\n",
    "\n",
    "Deep learning has potential to overcome the limitations of traditional machine learning and rule based systems by eliminating the task of describing explicit features or rules. Deep learning models looking at classifying the top 10 ICD-9 codes achieved relatively low performance (F1score: 0.37) suggesting that there is considerable room for improvement in both word representation and model architecture applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got our data from the MIMIC-III Critical Care Database, which contains data on over 40,000 ICU patients at Beth Isreal Deaconess Medical Center from 2001-2012. Though this database contains massive amounts of data on patient demographics, vital signs, procedures, practitioner notes, we focused on patient discharge notes and their associated ICD-9 codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the following three tables from the MIMIC-III Database:\n",
    "    \n",
    "* `D_ICD_Diagnoses`: this table contained the definitions & names of the ICD-9 codes. This was used so we could interpret which ICD-9 codes we were using.\n",
    "* `DIAGNOSES_ICD`: this table gave us the ICD-9 codes given to each patient at each visit. There is one row for each ICD-9 code for each patient's `HADM_ID` (admission ID).\n",
    "* `NOTEEVENTS`: this table gives has all the notes for the patients in the database. Notes also include those for echo, ECG, and radiology reports. We only used discharge notes for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify our network, we are only attempting to predict a subset of ICD-9 codes. We ran some numbers on the top (most common) ICD-9 codes in the code chunk below and decided to use the top 5 codes that didn't contain the \"V\" prefix. The \"V\" stands for Supplementary Classification of Factors Influencing Health Status and Contact with Health Services. Since they indicate conditions that influence care but do not necessarily represent the outcome of the visit, we decided to exclude those codes from our project.\n",
    "\n",
    "Note: because the entire table was so large, we ran a SQL query to get the counts of each ICD-9 code, which was saved into the csv file titled 'top100_dx.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count  ROW_ID ICD9_CODE               SHORT_TITLE  \\\n",
      "0  20703  4304.0      4019          Hypertension NOS   \n",
      "1  13111  4473.0      4280                   CHF NOS   \n",
      "2  12891  4462.0     42731       Atrial fibrillation   \n",
      "3  12429  4374.0     41401  Crnry athrscl natve vssl   \n",
      "4   9119  5908.0      5849  Acute kidney failure NOS   \n",
      "\n",
      "                                          LONG_TITLE  \n",
      "0                 Unspecified essential hypertension  \n",
      "1              Congestive heart failure, unspecified  \n",
      "2                                Atrial fibrillation  \n",
      "3  Coronary atherosclerosis of native coronary ar...  \n",
      "4                  Acute kidney failure, unspecified  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read in data\n",
    "top_codes = pd.read_csv('/Users/katherine/Desktop/BST 261/Final Project/data/top100_dx.csv',header=None)\n",
    "diags = pd.read_csv('/Users/katherine/Desktop/BST 261/Final Project/data/D_ICD_Diagnoses.csv')\n",
    "\n",
    "# merge tables\n",
    "df=pd.merge(top_codes,diags,left_on=0, right_on='ICD9_CODE',how='left')[:30]\n",
    "df=df.rename(columns={0:\"ICD\", 1:\"count\"})\n",
    "\n",
    "# removed ICD-9 codes starting with 'V'\n",
    "df=df[~df['ICD'].str.contains(\"V\")]\n",
    "print(df.iloc[:5,1:])\n",
    "\n",
    "ICD_CODES = [\"4019\",\"4280\",\"42731\",\"41401\",\"5849\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we found the five ICD-9 codes we wanted to predict, the next step was to subset the data and do the cleaning & preprocessing to get it ready to be inputted into our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Data Cleaning/Preprocessing\n",
    "\n",
    "##### 2.2.1 Fetch Embeddings & Notes Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = Word2Vec.load('./data/w2v_embeddings')\n",
    "# word_vectors = FastText.load('fasttext_embeddings')\n",
    "embedding_dim = word_vectors.wv.vectors.shape[1]\n",
    "\n",
    "'''\n",
    "**ONLY RUN IF NEED TO RE-PROCESS TEXT**\n",
    "Read in notes and pre-process text\n",
    "Also fetch HADM_IDs\n",
    "'''\n",
    "hadm_id = []\n",
    "input_notes = []\n",
    "with open(NOTEEVENTS, \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for i, note in enumerate(reader):\n",
    "        if note[6] == 'Discharge summary':\n",
    "            hadm_id.append(note[2])\n",
    "            note = preprocess_text2(note[-1])\n",
    "            input_notes.append(note)\n",
    "            \n",
    "'''\n",
    "Save processed input notes\n",
    "'''\n",
    "with open(\"processed_input_notes.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(input_notes)\n",
    "    \n",
    "'''\n",
    "Only fetch HADM_IDs\n",
    "'''\n",
    "hadm_id = []\n",
    "with open(NOTEEVENTS, \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for i, note in enumerate(reader):\n",
    "        if note[6] == 'Discharge summary':\n",
    "            hadm_id.append(note[2])\n",
    "            \n",
    "'''\n",
    "Load processed input notes \n",
    "'''\n",
    "input_notes = []\n",
    "with open(\"processed_input_notes.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for note in reader:\n",
    "        input_notes.append(note)\n",
    "deleted_dupes = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the note lengths is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1MF7HEypxGZ9yqgFUvZFsXgMMJrFPMrTc\" alt=\"Note Length\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prompted us to choose a sequence length of 3000 per summary for our purpose, considering a good analytical potential, computational resource availability and time constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency of the ICD-9 codes in our data is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1gBOmHQLXDDrjniphAcQtz9cTs6Opk9D8\" alt=\"Codes\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We narrowed down to the top 4 ICD-9 codes based on this distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Labels Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to predict the top 5 ICD-9 codes, so the outcomes are categorical. We'll need to feed 5x1 vectors to the neural network, where each ICD-9 code is represented by either a 0 if the patient was not assigned the code or 1 if the patient was assigned the code. \n",
    "\n",
    "We also filtered out any notes that were not discharge summaries since we want to predict ICD-9 codes that were assigned to patients after their ICU visit. There were also many visits that had multiple notes, meaning there were many `HADM_ID`'s with multiple occurrences in the table. Notes sharing the same `HADM_ID` required some extra consideration as well. There were a couple options we considered:\n",
    "\n",
    "* concatenating/merging notes into 1 large note per `HADM_ID`\n",
    "* keeping only the lastest note for the patient. The idea behind this was that there may be duplicate information in the notes, but the latest note would likely summarize the patient's condition the best.\n",
    "* removing these duplicate `HADM_ID`'s all together.\n",
    "\n",
    "Any admissions (`HADM_ID`'s) that appeared multiple times in the dataset (i.e. visits with multiple discharge summary notes) were dropped to simplify the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(DIAGNOSES_ICD)\n",
    "df2 = pd.read_csv(NOTEEVENTS)\n",
    "\n",
    "# Translate ICD codes to indicator variables\n",
    "dummy = pd.get_dummies(df1['ICD9_CODE'])[ICD_CODES]\n",
    "# Append indicator var columnns to original ICD df\n",
    "dummy_combined = pd.concat([df1, dummy], axis=1)\n",
    "# Combine by HADM_ID and drop columns\n",
    "dummy_combined = dummy_combined.groupby(['HADM_ID'], as_index=False).sum().drop(['ROW_ID','SUBJECT_ID','SEQ_NUM'], axis = 1)\n",
    "\n",
    "# now join the two tables together \n",
    "df_final = pd.merge(df2, dummy_combined,left_on=\"HADM_ID\", right_on='HADM_ID', how='left')\n",
    "# Filter by discharge summary\n",
    "df_final = df_final[df_final['CATEGORY'] == 'Discharge summary']\n",
    "# removed any hadmid that have more than one entry in database\n",
    "df_final = df_final.drop_duplicates(subset = \"HADM_ID\", keep = False)\n",
    "df_final = df_final.drop(['ROW_ID', 'SUBJECT_ID', 'CHARTDATE', \n",
    "                          'CHARTTIME', 'STORETIME', 'CATEGORY',\n",
    "                         'DESCRIPTION', 'CGID', 'ISERROR'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3 Subsampling the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly subsampled the data to allow us to train our neural network faster under our time constraints. The data was randomly sampled to ensure that the network was trained on some data that did not have any of the top 5 ICD-9 codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random sample of the data\n",
    "sub_df_final = df_final.sample(10000)\n",
    "\n",
    "input_notes = []\n",
    "hadm_id = []\n",
    "y = []\n",
    "for i in range(len(sub_df_final)):\n",
    "    y.append(list(map(int, sub_df_final.iloc[i, 2:].tolist())))\n",
    "    input_notes.append(preprocess_text2(sub_df_final.iloc[i, 1]))\n",
    "    hadm_id.append(sub_df_final.iloc[i, 0])\n",
    "y = np.array(y)\n",
    "\n",
    "'''\n",
    "Train/test split\n",
    "'''\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_notes, y, test_size=0.2)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "'''\n",
    "Takes words and converts to indices (which then correspond to vectors in\n",
    "the embedding models)\n",
    "'''\n",
    "#max_words = len(word_vectors.wv.vocab)\n",
    "max_words = 15000\n",
    "token = Tokenizer(max_words)\n",
    "token.fit_on_texts(input_notes)\n",
    "vocab_size = max_words + 1\n",
    "\n",
    "sequences = token.texts_to_sequences(x_train)\n",
    "test_sequences = token.texts_to_sequences(x_test)\n",
    "\n",
    "'''\n",
    "Convert to padded sequences\n",
    "'''\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "seq_len = 3000\n",
    "X = pad_sequences(sequences, maxlen=seq_len)\n",
    "X_test = pad_sequences(test_sequences, maxlen=seq_len)\n",
    "\n",
    "embeddings_index = {}\n",
    "vocab = token.word_index.keys()\n",
    "for word in vocab:\n",
    "    if word in word_vectors.wv.vocab:\n",
    "        coefs = np.asarray(word_vectors.wv[word], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "word_index = token.word_index\n",
    "embedding_matrix = np.zeros((vocab_size , embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "num_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gated Recurrent Network (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU's are a relatively new model similar to LSTM's in that it solves vanishing gradient problem from standard RNNs. GRU's combine the 'forget' & 'input' gates used in LSTM to an 'update' gate. The network then uses an 'update' and 'reset' gate to decide what information is kept as the model is being trained.\n",
    "\n",
    "It is said that GRU's and LSTM's usually have similar performance, but GRU's tend to be faster to train and easier to use. This is why we decided to use GRU's first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from keras.models import load_model\n",
    "\n",
    "m = load_model('model_20180501_liam')\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Liam GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "from timeit import default_timer as timer\n",
    "import gensim\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, Input, Dropout, Bidirectional, GaussianNoise\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#######################################################\n",
    "'''\n",
    "Edit these constants to change important things in code below\n",
    "'''\n",
    "NOTEEVENTS = \"./data/NOTEEVENTS.csv\"\n",
    "DIAGNOSES_ICD = \"./data/DIAGNOSES_ICD.csv\"\n",
    "EMBEDDING_MODEL = 'w2v' # 'w2v' or 'ft'\n",
    "ICD_CODES = [\"4019\",\"4280\",\"42731\", \"41401\", \"5849\"] # ICD codes of interest\n",
    "SEQ_LEN = 3000 # Max length of note\n",
    "MAX_WORDS = 50000\n",
    "\n",
    "SUBSAMPLE_SIZE = 20000\n",
    "TEST_SET_FRACTION = 0.2\n",
    "BATCH_SIZE = 128\n",
    "TRAINING_EPOCHS = 5\n",
    "MODEL_SAVE_PATH = './models/model_20180502_01_liam' # Path to save trained model to\n",
    "#######################################################\n",
    "\n",
    "\n",
    "def clean_string1(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    text = text.strip().lower().replace('-', '_').replace('.', '_').replace(' ', '_').rstrip('_')\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text2(query):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    query = re.sub('\\d+|\\d+\\.\\d+', '[NUM]', query)\n",
    "    query = re.sub('(\\[\\*\\*.*?\\*\\*\\])', '[PHI]', query)\n",
    "    query = query.strip('\"').strip('?').strip(\"'\").strip('(').strip(')').strip(':')\n",
    "    query = re.sub('['+'!\"#$%&\\'()*+,-./:;<=>?@\\\\^`{|}~'+']', '', query)\n",
    "    word_list = query.split()\n",
    "    word_list = [clean_string1(word) for word in word_list]\n",
    "    return word_list\n",
    "\n",
    "\n",
    "if EMBEDDING_MODEL == 'w2v':\n",
    "    word_vectors = Word2Vec.load('./data/w2v_embeddings')\n",
    "else:\n",
    "    word_vectors = FastText.load('./data/fasttext_embeddings')\n",
    "embedding_dim = word_vectors.wv.vectors.shape[1]\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(DIAGNOSES_ICD)\n",
    "df2 = pd.read_csv(NOTEEVENTS)\n",
    "# Translate ICD codes to indicator variables\n",
    "dummy = pd.get_dummies(df1['ICD9_CODE'])[ICD_CODES]\n",
    "# Append indicator var columnns to original ICD df\n",
    "dummy_combined = pd.concat([df1, dummy], axis=1)\n",
    "# Combine by HADM_ID and drop columns\n",
    "dummy_combined = dummy_combined.groupby(['HADM_ID'], as_index=False).sum().drop(['ROW_ID','SUBJECT_ID','SEQ_NUM'], axis = 1)\n",
    "#now join the two tables together\n",
    "df_final = pd.merge(df2, dummy_combined,left_on=\"HADM_ID\", right_on='HADM_ID', how='left')\n",
    "# Filter by discharge summary\n",
    "df_final = df_final[df_final['CATEGORY'] == 'Discharge summary']\n",
    "# removed any hadmid that have more than one entry in database\n",
    "df_final = df_final.drop_duplicates(subset = \"HADM_ID\", keep = False)\n",
    "df_final = df_final.drop(['ROW_ID', 'SUBJECT_ID', 'CHARTDATE',\n",
    "                          'CHARTTIME', 'STORETIME', 'CATEGORY',\n",
    "                         'DESCRIPTION', 'CGID', 'ISERROR'], axis = 1)\n",
    "#random sample of the data\n",
    "sub_df_final = df_final.sample(SUBSAMPLE_SIZE)\n",
    "\n",
    "# Read in and process data\n",
    "input_notes = []\n",
    "hadm_id = []\n",
    "y = []\n",
    "for i in range(len(sub_df_final)):\n",
    "    y.append(list(map(int, sub_df_final.iloc[i, 2:].tolist())))\n",
    "    input_notes.append(preprocess_text2(sub_df_final.iloc[i, 1]))\n",
    "    hadm_id.append(sub_df_final.iloc[i, 0])\n",
    "y = np.array(y)\n",
    "\n",
    "'''\n",
    "Train/test split\n",
    "'''\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_notes, y, test_size=TEST_SET_FRACTION)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "'''\n",
    "Takes words and converts to indices (which then correspond to vectors in\n",
    "the embedding models)\n",
    "'''\n",
    "#max_words = len(word_vectors.wv.vocab)\n",
    "max_words = MAX_WORDS\n",
    "token = Tokenizer(max_words)\n",
    "token.fit_on_texts(input_notes)\n",
    "vocab_size = max_words + 1\n",
    "\n",
    "sequences = token.texts_to_sequences(x_train)\n",
    "test_sequences = token.texts_to_sequences(x_test)\n",
    "\n",
    "'''\n",
    "Convert to padded sequences\n",
    "'''\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "seq_len = SEQ_LEN\n",
    "X = pad_sequences(sequences, maxlen=seq_len)\n",
    "X_test = pad_sequences(test_sequences, maxlen=seq_len)\n",
    "\n",
    "embeddings_index = {}\n",
    "vocab = token.word_index.keys()\n",
    "for word in vocab:\n",
    "    if word in word_vectors.wv.vocab:\n",
    "        coefs = np.asarray(word_vectors.wv[word], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "word_index = token.word_index\n",
    "embedding_matrix = np.zeros((vocab_size , embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "          if embedding_vector is not None:\n",
    "              # words not found in embedding index will be all-zeros.\n",
    "              embedding_matrix[i] = embedding_vector\n",
    "\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculates the precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def calculate_class_weights(Y):\n",
    "    '''\n",
    "    input: np array of labels\n",
    "    output: dict of multilabel class weights\n",
    "    '''\n",
    "    l_weights = []\n",
    "    c_weights = []\n",
    "    for i in range(y.shape[1]):\n",
    "        neg = len(y[y[:,i] == 0, i])\n",
    "        pos = len(y) - neg\n",
    "        neg_ratio = neg / pos\n",
    "        l_weights.append(neg_ratio)\n",
    "        c_weights.append({0: 1, 1: neg_ratio})\n",
    "    return tf.constant(l_weights), c_weights\n",
    "\n",
    "loss_weights, class_weights = calculate_class_weights(y_train)\n",
    "\n",
    "# config = K.tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = K.tf.Session(config=config)\n",
    "\n",
    "def _to_tensor(x, dtype):\n",
    "    \"\"\"Convert the input `x` to a tensor of type `dtype`.\n",
    "    # Arguments\n",
    "        x: An object to be converted (numpy array, list, tensors).\n",
    "        dtype: The destination type.\n",
    "    # Returns\n",
    "        A tensor.\n",
    "    \"\"\"\n",
    "    return tf.convert_to_tensor(x, dtype=dtype)\n",
    "\n",
    "\n",
    "def weighted_binary_crossentropy(target, output, from_logits=False):\n",
    "  \"\"\"Binary crossentropy between an output tensor and a target tensor.\n",
    "  Arguments:\n",
    "      output: A tensor.\n",
    "      target: A tensor with the same shape as `output`.\n",
    "      from_logits: Whether `output` is expected to be a logits tensor.\n",
    "          By default, we consider that `output`\n",
    "          encodes a probability distribution.\n",
    "  Returns:\n",
    "      A tensor.\n",
    "  \"\"\"\n",
    "  # Note: nn.softmax_cross_entropy_with_logits\n",
    "  # expects logits, Keras expects probabilities.\n",
    "      if not from_logits:\n",
    "        # transform back to logits\n",
    "        epsilon = _to_tensor(K.epsilon(), output.dtype.base_dtype)\n",
    "        output = tf.clip_by_value(output, epsilon, 1 - epsilon)\n",
    "        output = tf.log(output / (1 - output))\n",
    "    return tf.nn.weighted_cross_entropy_with_logits(target, output, pos_weight=loss_weights)\n",
    "\n",
    "\n",
    "from keras.layers import Lambda\n",
    "ClipLayer = Lambda(lambda x: K.clip(x, min_value=0.01, max_value=0.99))\n",
    "\n",
    "## Build the model ##\n",
    "input = Input(shape=(seq_len,))\n",
    "x = Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False)(input)\n",
    "x = GaussianNoise(0.75)(x)\n",
    "x = Bidirectional(GRU(units=128, recurrent_dropout=0.2, dropout=0.2, activation='relu', return_sequences=True))(x)\n",
    "x = Bidirectional(GRU(units=128, recurrent_dropout=0.2, dropout=0.2, activation='relu'))(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(num_classes, activation='sigmoid')(x)\n",
    "x = ClipLayer(x)\n",
    "model = Model(input, x)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=weighted_binary_crossentropy,\n",
    "              weighted_metrics=['binary_accuracy', precision, recall])\n",
    "\n",
    "model.fit(X, y_train,\n",
    "          epochs=TRAINING_EPOCHS,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          class_weight=class_weights)\n",
    "\n",
    "model.save(MODEL_SAVE_PATH)\n",
    "\n",
    "model.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model initially produced an accuracy of 74%. However, it convereged very quickly and did not improve with additional training. We were skeptical of our high accuracy, given our slightly imbalanced dataset, so we decided to look at a different metric - Precision/Recall. We generated a recall metric < 0.01, indicating that our model was simply predicting “0” for each diagnosis for nearly every patient. \n",
    "\n",
    "We implemented changes in our network architecture to try to improve recall, which did see some light, but the loss quickly converged around 1.03. We suspect this to have been caused by a vanishing gradient. GRUs are structurally fit towards but tackling the vanishing gradient problem, but from our approach to the problem, we believe that it is still affecting the outcome in spite of the model being based on a GRU. A further analysis is required to identify the crux of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work \n",
    "\n",
    "#### One-Dimensional CNN\n",
    "A 1-D CNN can extract key features of the data. We can expect this model to have vanishing gradient issues (the model would likely be unable to catch long term meaning throughout the notes) but the CNN may be better at picking up keywords. Implementing this would require more computational resources and time.\n",
    "\n",
    "#### Additional Considerations\n",
    "We had to limit the vector length of out discharge summaries due to computational resource and time limitations. A less restrictive trimming of word vectors for each discharge note could have possibly improved the results. Also,\n",
    "a choice of more ICD-9 codes as potential prediction classes to tend towards the original data distribution\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
