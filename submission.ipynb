{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BST 261: Final Project\n",
    "\n",
    "## Subtitle\n",
    "\n",
    "\n",
    "### 1. Overview \n",
    "\n",
    "\n",
    "#### 1.1 Motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Related Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got our data from the MIMIC-III Critical Care Database, which contains data on over 40,000 ICU patients at Beth Isreal Deaconess Medical Center from 2001-2012. Though this database contains massive amounts of data on patient demographics, vital signs, procedures, practitioner notes, we focused on patient discharge notes and their associated ICD-9 codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the following three tables from the MIMIC-III Database:\n",
    "    \n",
    "* `D_ICD_Diagnoses`: this table contained the definitions & names of the ICD-9 codes. This was used so we could interpret which ICD-9 codes we were using.\n",
    "* `DIAGNOSES_ICD`: this table gave us the ICD-9 codes given to each patient at each visit. There is one row for each ICD-9 code for each patient's `HADM_ID` (admission ID).\n",
    "* `NOTEEVENTS`: this table gives has all the notes for the patients in the database. Notes also include those for echo, ECG, and radiology reports. We only used discharge notes for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify our network, we are only attempting to predict a subset of ICD-9 codes. We ran some numbers on the top (most common) ICD-9 codes in the code chunk below and decided to use the top 5 codes that didn't contain the \"V\" prefix. The \"V\" stands for Supplementary Classification of Factors Influencing Health Status and Contact with Health Services. Since they indicate conditions that influence care but do not necessarily represent the outcome of the visit, we decided to exclude those codes from our project.\n",
    "\n",
    "Note: because the entire table was so large, we ran a SQL query to get the counts of each ICD-9 code, which was saved into the csv file titled 'top100_dx.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count  ROW_ID ICD9_CODE               SHORT_TITLE  \\\n",
      "0  20703  4304.0      4019          Hypertension NOS   \n",
      "1  13111  4473.0      4280                   CHF NOS   \n",
      "2  12891  4462.0     42731       Atrial fibrillation   \n",
      "3  12429  4374.0     41401  Crnry athrscl natve vssl   \n",
      "4   9119  5908.0      5849  Acute kidney failure NOS   \n",
      "\n",
      "                                          LONG_TITLE  \n",
      "0                 Unspecified essential hypertension  \n",
      "1              Congestive heart failure, unspecified  \n",
      "2                                Atrial fibrillation  \n",
      "3  Coronary atherosclerosis of native coronary ar...  \n",
      "4                  Acute kidney failure, unspecified  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read in data\n",
    "top_codes = pd.read_csv('/Users/katherine/Desktop/BST 261/Final Project/data/top100_dx.csv',header=None)\n",
    "diags = pd.read_csv('/Users/katherine/Desktop/BST 261/Final Project/data/D_ICD_Diagnoses.csv')\n",
    "\n",
    "# merge tables\n",
    "df=pd.merge(top_codes,diags,left_on=0, right_on='ICD9_CODE',how='left')[:30]\n",
    "df=df.rename(columns={0:\"ICD\", 1:\"count\"})\n",
    "\n",
    "# removed ICD-9 codes starting with 'V'\n",
    "df=df[~df['ICD'].str.contains(\"V\")]\n",
    "print(df.iloc[:5,1:])\n",
    "\n",
    "ICD_CODES = [\"4019\",\"4280\",\"42731\",\"41401\",\"5849\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we found the five ICD-9 codes we wanted to predict, the next step was to subset the data and do the cleaning & preprocessing to get it ready to be inputted into our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Data Cleaning/Preprocessing\n",
    "\n",
    "##### 2.2.1 Fetch Embeddings & Notes Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Liam's Section!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = Word2Vec.load('./data/w2v_embeddings')\n",
    "# word_vectors = FastText.load('fasttext_embeddings')\n",
    "embedding_dim = word_vectors.wv.vectors.shape[1]\n",
    "\n",
    "'''\n",
    "**ONLY RUN IF NEED TO RE-PROCESS TEXT**\n",
    "Read in notes and pre-process text\n",
    "Also fetch HADM_IDs\n",
    "'''\n",
    "hadm_id = []\n",
    "input_notes = []\n",
    "with open(NOTEEVENTS, \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for i, note in enumerate(reader):\n",
    "        if note[6] == 'Discharge summary':\n",
    "            hadm_id.append(note[2])\n",
    "            note = preprocess_text2(note[-1])\n",
    "            input_notes.append(note)\n",
    "            \n",
    "'''\n",
    "Save processed input notes\n",
    "'''\n",
    "with open(\"processed_input_notes.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(input_notes)\n",
    "    \n",
    "'''\n",
    "Only fetch HADM_IDs\n",
    "'''\n",
    "hadm_id = []\n",
    "with open(NOTEEVENTS, \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for i, note in enumerate(reader):\n",
    "        if note[6] == 'Discharge summary':\n",
    "            hadm_id.append(note[2])\n",
    "            \n",
    "'''\n",
    "Load processed input notes \n",
    "'''\n",
    "input_notes = []\n",
    "with open(\"processed_input_notes.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for note in reader:\n",
    "        input_notes.append(note)\n",
    "deleted_dupes = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Labels Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to predict the top 5 ICD-9 codes, so the outcomes are categorical. We'll need to feed 5x1 vectors to the neural network, where each ICD-9 code is represented by either a 0 if the patient was not assigned the code or 1 if the patient was assigned the code. \n",
    "\n",
    "We also filtered out any notes that were not discharge summaries since we want to predict ICD-9 codes that were assigned to patients after their ICU visit. There were also many visits that had multiple notes, meaning there were many `HADM_ID`'s with multiple occurrences in the table. Notes sharing the same `HADM_ID` required some extra consideration as well. There were a couple options we considered:\n",
    "\n",
    "* concatenating/merging notes into 1 large note per `HADM_ID`\n",
    "* keeping only the lastest note for the patient. The idea behind this was that there may be duplicate information in the notes, but the latest note would likely summarize the patient's condition the best.\n",
    "* removing these duplicate `HADM_ID`'s all together.\n",
    "\n",
    "Any admissions (`HADM_ID`'s) that appeared multiple times in the dataset (i.e. visits with multiple discharge summary notes) were dropped to simplify the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(DIAGNOSES_ICD)\n",
    "df2 = pd.read_csv(NOTEEVENTS)\n",
    "\n",
    "# Translate ICD codes to indicator variables\n",
    "dummy = pd.get_dummies(df1['ICD9_CODE'])[ICD_CODES]\n",
    "# Append indicator var columnns to original ICD df\n",
    "dummy_combined = pd.concat([df1, dummy], axis=1)\n",
    "# Combine by HADM_ID and drop columns\n",
    "dummy_combined = dummy_combined.groupby(['HADM_ID'], as_index=False).sum().drop(['ROW_ID','SUBJECT_ID','SEQ_NUM'], axis = 1)\n",
    "\n",
    "# now join the two tables together \n",
    "df_final = pd.merge(df2, dummy_combined,left_on=\"HADM_ID\", right_on='HADM_ID', how='left')\n",
    "# Filter by discharge summary\n",
    "df_final = df_final[df_final['CATEGORY'] == 'Discharge summary']\n",
    "# removed any hadmid that have more than one entry in database\n",
    "df_final = df_final.drop_duplicates(subset = \"HADM_ID\", keep = False)\n",
    "df_final = df_final.drop(['ROW_ID', 'SUBJECT_ID', 'CHARTDATE', \n",
    "                          'CHARTTIME', 'STORETIME', 'CATEGORY',\n",
    "                         'DESCRIPTION', 'CGID', 'ISERROR'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3 Subsampling the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly subsampled the data to allow us to train our neural network faster under our time constraints. The data was randomly sampled to ensure that the network was trained on some data that did not have any of the top 5 ICD-9 codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample of the data\n",
    "sub_df_final = df_final.sample(10000)\n",
    "\n",
    "input_notes = []\n",
    "hadm_id = []\n",
    "y = []\n",
    "for i in range(len(sub_df_final)):\n",
    "    y.append(list(map(int, sub_df_final.iloc[i, 2:].tolist())))\n",
    "    input_notes.append(preprocess_text2(sub_df_final.iloc[i, 1]))\n",
    "    hadm_id.append(sub_df_final.iloc[i, 0])\n",
    "y = np.array(y)\n",
    "\n",
    "'''\n",
    "Train/test split\n",
    "'''\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_notes, y, test_size=0.2)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "'''\n",
    "Takes words and converts to indices (which then correspond to vectors in\n",
    "the embedding models)\n",
    "'''\n",
    "#max_words = len(word_vectors.wv.vocab)\n",
    "max_words = 15000\n",
    "token = Tokenizer(max_words)\n",
    "token.fit_on_texts(input_notes)\n",
    "vocab_size = max_words + 1\n",
    "\n",
    "sequences = token.texts_to_sequences(x_train)\n",
    "test_sequences = token.texts_to_sequences(x_test)\n",
    "\n",
    "'''\n",
    "Convert to padded sequences\n",
    "'''\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "seq_len = 3000\n",
    "X = pad_sequences(sequences, maxlen=seq_len)\n",
    "X_test = pad_sequences(test_sequences, maxlen=seq_len)\n",
    "\n",
    "embeddings_index = {}\n",
    "vocab = token.word_index.keys()\n",
    "for word in vocab:\n",
    "    if word in word_vectors.wv.vocab:\n",
    "        coefs = np.asarray(word_vectors.wv[word], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "word_index = token.word_index\n",
    "embedding_matrix = np.zeros((vocab_size , embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "num_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 One-Dimensional CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with a 1-D CNN to extract key features of the data. We expect this model to have vanishing gradient issues (the model would likely be unable to catch long term meaning throughout the notes) but the CNN may be better at picking up keywords. Since we're not sure what is a better predictor of the ICD-9 codes, this is the model we started with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution: linear transformation of the input\n",
    "\n",
    "Detector: nonlinear activation (ReLu)\n",
    "\n",
    "Pooling: further modification (downsizing) of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Gated Recurrent Network (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU's are a relatively new model similar to LSTM's in that it solves vanishing gradient problem from standard RNNs. GRU's combine the 'forget' & 'input' gates used in LSTM to an 'update' gate. The network then uses an 'update' and 'reset' gate to decide what information is kept as the model is being trained.\n",
    "\n",
    "It is said that GRU's and LSTM's usually have similar performance, but GRU's tend to be faster to train and easier to use. This is why we decided to use GRU's first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from keras.models import load_model\n",
    "\n",
    "m = load_model('model_20180501_liam')\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Abhi CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Liam GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots to add:\n",
    "\n",
    "* %age of rows with the top 5 codes\n",
    "* distribution of length of notes\n",
    "* distribution of each code in our subsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
