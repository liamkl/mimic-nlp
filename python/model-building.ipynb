{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, global vars, functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Liam/anaconda3/envs/fastai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "from timeit import default_timer as timer\n",
    "import gensim\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, Input, Dropout, Bidirectional, GaussianNoise\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to check if GPU is working as intended\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEEVENTS = \"./data/NOTEEVENTS.csv\"\n",
    "DIAGNOSES_ICD = \"./data/DIAGNOSES_ICD.csv\"\n",
    "\n",
    "# record the top codes in a list\n",
    "ICD_CODES = [\"4019\",\"4280\",\"42731\", \"41401\", \"5849\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string1(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    text = text.strip().lower().replace('-', '_').replace('.', '_').replace(' ', '_').rstrip('_')\n",
    "    return text\n",
    "\n",
    "def preprocess_text2(query):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    query = re.sub('\\d+|\\d+\\.\\d+', '[NUM]', query)\n",
    "    query = re.sub('(\\[\\*\\*.*?\\*\\*\\])', '[PHI]', query)\n",
    "    query = query.strip('\"').strip('?').strip(\"'\").strip('(').strip(')').strip(':')\n",
    "    query = re.sub('['+'!\"#$%&\\'()*+,-./:;<=>?@\\\\^`{|}~'+']', '', query)\n",
    "    word_list = query.split()\n",
    "    word_list = [clean_string1(word) for word in word_list]\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch embeddings and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = Word2Vec.load('./data/w2v_embeddings')\n",
    "# word_vectors = FastText.load('fasttext_embeddings')\n",
    "embedding_dim = word_vectors.wv.vectors.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data [IGNORE - SEE SECTION BELOW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "**ONLY RUN IF NEED TO RE-PROCESS TEXT**\n",
    "Read in notes and pre-process text\n",
    "Also fetch HADM_IDs\n",
    "'''\n",
    "hadm_id = []\n",
    "input_notes = []\n",
    "with open(NOTEEVENTS, \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for i, note in enumerate(reader):\n",
    "        if note[6] == 'Discharge summary':\n",
    "            hadm_id.append(note[2])\n",
    "            note = preprocess_text2(note[-1])\n",
    "            input_notes.append(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Save processed input notes\n",
    "'''\n",
    "with open(\"processed_input_notes.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(input_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Only fetch HADM_IDs\n",
    "'''\n",
    "hadm_id = []\n",
    "with open(NOTEEVENTS, \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for i, note in enumerate(reader):\n",
    "        if note[6] == 'Discharge summary':\n",
    "            hadm_id.append(note[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load processed input notes \n",
    "'''\n",
    "input_notes = []\n",
    "with open(\"processed_input_notes.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for note in reader:\n",
    "        input_notes.append(note)\n",
    "deleted_dupes = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels [AND INPUT PROCESSING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Liam/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv(DIAGNOSES_ICD)\n",
    "df2 = pd.read_csv(NOTEEVENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate ICD codes to indicator variables\n",
    "dummy = pd.get_dummies(df1['ICD9_CODE'])[ICD_CODES]\n",
    "# Append indicator var columnns to original ICD df\n",
    "dummy_combined = pd.concat([df1, dummy], axis=1)\n",
    "# Combine by HADM_ID and drop columns\n",
    "dummy_combined = dummy_combined.groupby(['HADM_ID'], as_index=False).sum().drop(['ROW_ID','SUBJECT_ID','SEQ_NUM'], axis = 1)\n",
    "\n",
    "#now join the two tables together \n",
    "df_final = pd.merge(df2, dummy_combined,left_on=\"HADM_ID\", right_on='HADM_ID', how='left')\n",
    "# Filter by discharge summary\n",
    "df_final = df_final[df_final['CATEGORY'] == 'Discharge summary']\n",
    "# removed any hadmid that have more than one entry in database\n",
    "df_final = df_final.drop_duplicates(subset = \"HADM_ID\", keep = False)\n",
    "df_final = df_final.drop(['ROW_ID', 'SUBJECT_ID', 'CHARTDATE', \n",
    "                          'CHARTTIME', 'STORETIME', 'CATEGORY',\n",
    "                         'DESCRIPTION', 'CGID', 'ISERROR'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#random sample of the data\n",
    "sub_df_final = df_final.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_notes = []\n",
    "hadm_id = []\n",
    "y = []\n",
    "for i in range(len(sub_df_final)):\n",
    "    y.append(list(map(int, sub_df_final.iloc[i, 2:].tolist())))\n",
    "    input_notes.append(preprocess_text2(sub_df_final.iloc[i, 1]))\n",
    "    hadm_id.append(sub_df_final.iloc[i, 0])\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train/test split\n",
    "'''\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_notes, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 62520 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "'''\n",
    "Takes words and converts to indices (which then correspond to vectors in\n",
    "the embedding models)\n",
    "'''\n",
    "#max_words = len(word_vectors.wv.vocab)\n",
    "max_words = 15000\n",
    "token = Tokenizer(max_words)\n",
    "token.fit_on_texts(input_notes)\n",
    "vocab_size = max_words + 1\n",
    "\n",
    "sequences = token.texts_to_sequences(x_train)\n",
    "test_sequences = token.texts_to_sequences(x_test)\n",
    "\n",
    "'''\n",
    "Convert to padded sequences\n",
    "'''\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "seq_len = 3000\n",
    "X = pad_sequences(sequences, maxlen=seq_len)\n",
    "X_test = pad_sequences(test_sequences, maxlen=seq_len)\n",
    "\n",
    "embeddings_index = {}\n",
    "vocab = token.word_index.keys()\n",
    "for word in vocab:\n",
    "    if word in word_vectors.wv.vocab:\n",
    "      coefs = np.asarray(word_vectors.wv[word], dtype='float32')\n",
    "      embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = token.word_index\n",
    "embedding_matrix = np.zeros((vocab_size , embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size:\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "          # words not found in embedding index will be all-zeros.\n",
    "          embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "config = K.tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = K.tf.Session(config=config)\n",
    "\n",
    "from keras.layers import Lambda\n",
    "ClipLayer = Lambda(lambda x: K.clip(x, min_value=0.01, max_value=0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3072/8000 [==========>...................] - ETA: 12:56 - loss: 0.6023 - acc: 0.7070"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, Input, Dropout, Bidirectional, GaussianNoise\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "## Build the model ##\n",
    "input = Input(shape=(seq_len,))\n",
    "x = Embedding(input_dim = vocab_size , output_dim = embedding_dim, weights=[embedding_matrix], trainable=False)(input)\n",
    "x = GaussianNoise(0.75)(x)\n",
    "x = Bidirectional(GRU(units = 128, recurrent_dropout=0.2, dropout=0.2, activation = 'relu', return_sequences=True))(x)\n",
    "x = Bidirectional(GRU(units = 128, recurrent_dropout=0.2, dropout=0.2, activation = 'relu'))(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(num_classes, activation='sigmoid')(x)\n",
    "x = ClipLayer(x)\n",
    "model = Model(input,x)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X,y_train,epochs=5, batch_size = 128) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
