{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub('\\d+|\\d+\\.\\d+', '[NUM]', text)\n",
    "\n",
    "    # PHI tags are in format [**INFORMATION**]\n",
    "    text = re.sub('(\\[\\*\\*.*?\\*\\*\\])', '[PHI]', text)\n",
    "#     phi_tags = re.findall('(\\[\\*\\*.*?\\*\\*\\])', text)\n",
    "#     for i, tag in enumerate(phi_tags):\n",
    "#         text = text.replace(tag, '__PHI_{}__'.format(i))\n",
    "        \n",
    "    text = re.sub('---+', '\\n\\n-----\\n\\n', text)\n",
    "    text = re.sub('___+', '\\n\\n_____\\n\\n', text)\n",
    "    text = re.sub('[\\?\\.\\!]+(?=[\\?\\.\\!])', '', text)\n",
    "    text = re.sub('\\\\+', ' ', text)\n",
    "    text = re.sub('-|/', '_', text)\n",
    "    text = re.sub('_+', '_', text)\n",
    "    text = re.sub('\\*|\\(|\\)', ' ', text)\n",
    "    text = re.sub('['+'!\"#$%&\\'()*+,-./:;<=>?@\\\\^`{|}~'+']', '', text)\n",
    "    text = re.sub('\\s_\\s', ' ', text)\n",
    "    text = re.sub('\\r+|\\n+|\\u0085+|\\u2028+|\\u2029+', ' ', text) # Replace newlines\n",
    "    text = re.sub('\\s+|\\u00A0+', ' ', text) # Replace multiple spaces w/ single    \n",
    "    segments = text.split(' ')\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEEVENTS = \"../data/NOTEEVENTS.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = []\n",
    "with open(NOTEEVENTS, \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for i, note in enumerate(reader):\n",
    "        note = preprocess_text(note[-1])\n",
    "        notes.append(note)\n",
    "#         if note[6] == 'Discharge summary':\n",
    "#             note = preprocess_text(note[-1])\n",
    "#             notes.append(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.fasttext import FastText\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(size=300, window=10, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "\n",
    "model.train(lines, total_examples=len(lines), epochs=5)\n",
    "\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_model_gensim2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match(['cipro', 'dexamethasone', 'radiology', 'sitagliptin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"processed_text.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string1(text):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    text = text.strip().lower().replace('-', '_').replace('.', '_').replace(' ', '_').rstrip('_')\n",
    "    return text\n",
    "\n",
    "def preprocess_text2(query):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    query = re.sub('\\d+|\\d+\\.\\d+', '[NUM]', query)\n",
    "    query = re.sub('(\\[\\*\\*.*?\\*\\*\\])', '[PHI]', query)\n",
    "    query = query.strip('\"').strip('?').strip(\"'\").strip('(').strip(')').strip(':')\n",
    "    query = re.sub('['+'!\"#$%&\\'()*+,-./:;<=>?@\\\\^`{|}~'+']', '', query)\n",
    "    word_list = query.split()\n",
    "    word_list = [clean_string1(word) for word in word_list]\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes2 = []\n",
    "with open(NOTEEVENTS, \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for i, note in enumerate(reader):\n",
    "        note = preprocess_text2(note[-1])\n",
    "        notes2.append(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"processed_text.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(notes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes2 = []\n",
    "with open(\"processed_text.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for note in reader:\n",
    "        notes2.append(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes2[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = FastText(size=300, window=10, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "\n",
    "ft_model.build_vocab(notes2)\n",
    "\n",
    "end = timer()\n",
    "print(end-start)\n",
    "print(ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "\n",
    "ft_model.train(notes2, total_examples=len(notes2), epochs=5)\n",
    "\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.save('fasttext_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(size=300, window=10, min_count=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "\n",
    "w2v_model.build_vocab(notes2)\n",
    "\n",
    "end = timer()\n",
    "print(end-start)\n",
    "print(ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "\n",
    "w2v_model.train(notes2, total_examples=len(notes2), epochs=5)\n",
    "\n",
    "end = timer()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save('w2v_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing input data/fake labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "word_vectors = Word2Vec.load('w2v_embeddings')\n",
    "# word_vectors = FastText.load('fasttext_embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = word_vectors.wv.vectors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_notes = []\n",
    "with open(NOTEEVENTS, \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for i, note in enumerate(reader):\n",
    "        if note[6] == 'Discharge summary':\n",
    "            note = preprocess_text2(note[-1])\n",
    "            input_notes.append(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for local testing subset notes to 5000\n",
    "subset_notes = input_notes[:5000][:]\n",
    "\n",
    "N = len(subset_notes)\n",
    "# Create fake labels\n",
    "p = [0.4, 0.28, 0.22, 0.19, 0.16]\n",
    "y = np.array([[np.random.binomial(1, p[i]) for i in range(len(p))] for x in range(N)])\n",
    "# Create 70/30 train/test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(subset_notes, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 0\n",
    "for note in subset_notes:\n",
    "    if len(note) > MAXLEN:\n",
    "        MAXLEN = len(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "max_words = len(w2v_model.wv.vocab)\n",
    "max_words = 15000\n",
    "token = Tokenizer(max_words)\n",
    "token.fit_on_texts(subset_notes)\n",
    "vocab_size = max_words + 1\n",
    "\n",
    "sequences = token.texts_to_sequences(x_train)\n",
    "test_sequences = token.texts_to_sequences(x_test)\n",
    "## Convert to sequences ##\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "seq_len = 3000\n",
    "X = pad_sequences(sequences, maxlen=seq_len)\n",
    "X_test = pad_sequences(test_sequences, maxlen=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46242 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "vocab = token.word_index.keys()\n",
    "for word in vocab:\n",
    "    if word in w2v_model.wv.vocab:\n",
    "      coefs = np.asarray(w2v_model.wv[word], dtype='float32')\n",
    "      embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = token.word_index\n",
    "embedding_matrix = np.zeros((vocab_size , embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size:\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "          # words not found in embedding index will be all-zeros.\n",
    "          embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, Input, Dropout, Bidirectional, GaussianNoise\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "\n",
    "## Build the model ##\n",
    "input = Input(shape=(seq_len,))\n",
    "x = Embedding(input_dim = vocab_size , output_dim = embedding_dim, weights=[embedding_matrix], trainable=False)(input)\n",
    "x = GaussianNoise(0.75)(x)\n",
    "x = Bidirectional(GRU(units = 128, recurrent_dropout=0.2, dropout=0.2, activation = 'relu', return_sequences=True))(x)\n",
    "x = Bidirectional(GRU(units = 128, recurrent_dropout=0.2, dropout=0.2, activation = 'relu'))(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(num_classes, activation='sigmoid')(x)\n",
    "model = Model(input,x)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X,y_train,epochs=25, batch_size = 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#Chris Hilger's code\n",
    "\n",
    "# Import all libraries needed for the tutorial\n",
    "\n",
    "# General syntax to import specific functions in a library: \n",
    "##from (library) import (specific library function)\n",
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "# General syntax to import a library but no functions: \n",
    "##import (library) as (give the library a nickname/alias)\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd #this is how I usually import pandas\n",
    "import sys #only needed to determine Python version number\n",
    "import matplotlib #only needed to determine Matplotlib version number\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "df1 = pd.read_csv(\"../data/DIAGNOSES_ICD.csv\")\n",
    "df2 = pd.read_csv(\"../data/NOTEEVENTS.csv\")\n",
    "\n",
    "# record the top codes in a list\n",
    "codes = [\"4019\",\"4280\",\"42731\", \"41401\", \"5849\"]\n",
    "\n",
    "#make reduced version of df1 to only contain rows that have top icd9 cods\n",
    "df1_reduced = df1[df1[\"ICD9_CODE\"].isin(codes)] \n",
    "\n",
    "dummy = pd.get_dummies(df1_reduced['ICD9_CODE'])\n",
    "\n",
    "#combine the dummy df with the reduced df1 matrix\n",
    "dummy_combined = pd.concat([df1_reduced, dummy],axis=1)\n",
    "\n",
    "dummy_combined.head(10)\n",
    "#dummy_combined.shape\n",
    "\n",
    "#now drop unused columns\n",
    "dummy_combined_reduced = dummy_combined.drop(['ROW_ID','SUBJECT_ID','SEQ_NUM','ICD9_CODE'], axis = 1)\n",
    "dummy_combined_reduced.head(10)\n",
    "\n",
    "#now filter to get single instances of HADM_ID\n",
    "dcr_final = dummy_combined_reduced.drop_duplicates(subset = \"HADM_ID\", keep = \"first\")\n",
    "\n",
    "#now join the two tables together \n",
    "df_final=pd.merge(df2,dcr_final,left_on=\"HADM_ID\", right_on='HADM_ID',how='left')\n",
    "\n",
    "#remove NAs from the data, (for doctors notes that were for mapped to nontop5 icd9 codes)\n",
    "df_final = df_final[np.isfinite(df_final['4019'])]\n",
    "\n",
    "\n",
    "#df_final.to_csv(\"top_icd9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(df1['ICD9_CODE'])[codes]\n",
    "dummy_combined = pd.concat([df1, dummy], axis=1)\n",
    "dummy_combined = dummy_combined.groupby(['HADM_ID'], as_index=False).sum().drop(['ROW_ID','SUBJECT_ID','SEQ_NUM'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>4019</th>\n",
       "      <th>4280</th>\n",
       "      <th>42731</th>\n",
       "      <th>41401</th>\n",
       "      <th>5849</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100007</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100009</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID  4019  4280  42731  41401  5849\n",
       "0   100001     0     0      0      0     1\n",
       "1   100003     1     0      0      0     0\n",
       "2   100006     0     0      0      0     0\n",
       "3   100007     1     0      0      0     0\n",
       "4   100009     1     0      0      1     0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59652"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(discharge_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58976"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dummy_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge_df[discharge_df.duplicated('HADM_ID')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge_df.loc[60400]['TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
